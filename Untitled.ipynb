{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32af51d-802c-4905-889f-9c8499a0952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1e2d20-5cb4-4706-8246-b298bc1aac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 00:45:09 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-09-13 00:45:09 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-6.8.0-40-generic-x86_64-with-glibc2.35\n",
      "2024-09-13 00:45:09 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-09-13 00:45:09 [py.warnings] WARNING: /home/aditya/.local/lib/python3.10/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-09-13 00:45:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2024-09-13 00:45:09 [scrapy.extensions.telnet] INFO: Telnet Password: 950ff80884de9e2f\n",
      "2024-09-13 00:45:09 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-09-13 00:45:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-09-13 00:45:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-09-13 00:45:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-09-13 00:45:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-09-13 00:45:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-09-13 00:45:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-09-13 00:45:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2024-09-13 00:45:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.livemint.com/companies/news/swiss-authorities-freeze-over-310-million-funds-across-accounts-related-to-adani-investigation-hindenburg-research-11726161818402.html> (referer: None)\n",
      "2024-09-13 00:45:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.livemint.com/companies/news/swiss-authorities-freeze-over-310-million-funds-across-accounts-related-to-adani-investigation-hindenburg-research-11726161818402.html>\n",
      "{'Article URL': 'https://www.livemint.com/companies/news/swiss-authorities-freeze-over-310-million-funds-across-accounts-related-to-adani-investigation-hindenburg-research-11726161818402.html', 'Title': 'Swiss authorities freeze over $310 million funds across accounts related to Adani investigation: Hindenburg Research', 'Author Name': ' ', 'Author URL': 'Not available', 'Article Content': \"US-based short seller   alleged that Swiss authorities have frozen more than $310 million in funds across several Swiss Bank accounts linked to the   and securities  of Adani Group, according to the firm's social media post on platform X, on September 12. “Swiss authorities have frozen more than $310 million in  across multiple Swiss bank accounts as part of a money laundering and securities forgery investigation into  , dating back as early as 2021,” said the US-based short seller in its post on platform X, on Thursday night.\\xa0 , in its post, cited a Swiss media outlet, Gotham City, claiming that an order from the Federal Criminal Court (FCC) revealed that the Geneva Public Prosecutor's office was investigating the alleged wrongdoing of the   conglomerate “well before activist investors from Hindenburg Research made its first accusations.” The report also alleged that more than $310 million belonging to a frontman working for billionaire   is sequestered (hidden) in six Swiss banks. The report also mentioned that the Office of the Attorney General of Switzerland (OAG) took over the investigation after the case was revealed in front of the press. The short seller mentioned that the alleged frontman invested in “opaque BVI/Mauritius & Bermuda funds,” which nearly just own Adani group's stocks, citing the court records released by the Swiss  court, reported in the local Swiss media outlet. “Prosecutors detailed how an Adani frontman invested in opaque BVI/Mauritius & Bermuda  that almost exclusively owned Adani stocks, according to newly released Swiss criminal court records reported by Swiss media outlet,” said Hindenburg Research in its post on platform X. This comes as the latest development in the  , which originated from the original  report on the Adani Group named as “Adani Group: How The World’s 3rd Richest Man Is Pulling The Largest Con In Corporate History.” The short-seller released the report on January 24, 2023.\\xa0 Adani Enterprises shares closed 1.84 per cent higher at   2,991.40 after Thursday's market close, compared to   2,937.35 in the previous day. Hindenburg's post came after market operating hours.\\xa0 \\xa0\", 'Published Date': '12 Sep 2024, 11:13 PM IST'}\n",
      "2024-09-13 00:45:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-09-13 00:45:10 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: articles.json\n",
      "2024-09-13 00:45:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 535,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 139127,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.693773,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 9, 12, 19, 15, 10, 519074, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 731403,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 144044032,\n",
      " 'memusage/startup': 144044032,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 9, 12, 19, 15, 9, 825301, tzinfo=datetime.timezone.utc)}\n",
      "2024-09-13 00:45:10 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'article_spider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = \"https://www.livemint.com/companies/news/swiss-authorities-freeze-over-310-million-funds-across-accounts-related-to-adani-investigation-hindenburg-research-11726161818402.html\"\n",
    "        yield scrapy.Request(url, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        })\n",
    "        \n",
    "    def parse(self, response):\n",
    "\n",
    "        # Extract the title (assuming it's in the <h1> tag)\n",
    "        title = response.xpath('//h1/text()').get()\n",
    "\n",
    "        # Extract the author name inside the <strong> tag\n",
    "        author_name = response.xpath('//div[contains(@class, \"storyPage_authorDesc__zPjwo\")]/a/strong/text()').get()\n",
    "\n",
    "        # Extract the author URL from the href attribute of the <a> tag\n",
    "        author_url = response.xpath('//div[contains(@class, \"storyPage_authorDesc__zPjwo\")]/a/@href').get()\n",
    "\n",
    "        # Extract the published/updated date from the <span> tag\n",
    "        published_date = response.xpath('//div[contains(@class, \"storyPage_date__JS9qJ\")]//span/text()').get()\n",
    "        \n",
    "        # Extract article content\n",
    "        article_content = ' '.join(response.css('.storyPage_storyContent__m_MYl p::text').getall())\n",
    "        \n",
    "        # Article URL is available from the response\n",
    "        article_url = response.url\n",
    "        \n",
    "        yield {\n",
    "            \"Article URL\": article_url,\n",
    "            \"Title\": title,\n",
    "            \"Author Name\": author_name,\n",
    "            \"Author URL\": author_url,\n",
    "            \"Article Content\": article_content,\n",
    "            \"Published Date\": published_date\n",
    "        }\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    \"FEEDS\": {\n",
    "        \"articles.json\": {\"format\": \"json\"},\n",
    "    },\n",
    "})\n",
    "\n",
    "process.crawl(ArticleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e9385b-c931-447f-b2c0-88435b5f94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 00:39:08 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-09-13 00:39:08 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-6.8.0-40-generic-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25d7c3a-679c-4f45-b575-4864189bc28a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url_to_scrape \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.livemint.com/news/world/joe-biden-to-host-pm-modi-other-quad-leaders-in-delaware-on-september-21-11726160475837.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241m.\u001b[39mcrawl(ArticleSpider)\n\u001b[1;32m      3\u001b[0m process\u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process' is not defined"
     ]
    }
   ],
   "source": [
    "url_to_scrape = \"https://www.livemint.com/news/world/joe-biden-to-host-pm-modi-other-quad-leaders-in-delaware-on-september-21-11726160475837.html\"\n",
    "process.crawl(ArticleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2d3372-9e36-4788-abc1-e74ae9986a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = \"example\"\n",
    "    start_urls = ['https://www.livemint.com/companies/birla-estates-buys-land-from-hindalco-industries-for-rs-537-42-crore-in-mumbai-11726159806501.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract the title (assuming it's in the <h1> tag)\n",
    "        title = response.xpath('//h1/text()').get()\n",
    "\n",
    "        # Extract the author name inside the <strong> tag\n",
    "        author_name = response.xpath('//div[contains(@class, \"storyPage_authorDesc__zPjwo\")]/a/strong/text()').get()\n",
    "\n",
    "        # Extract the author URL from the href attribute of the <a> tag\n",
    "        author_url = response.xpath('//div[contains(@class, \"storyPage_authorDesc__zPjwo\")]/a/@href').get()\n",
    "\n",
    "        # Extract the published/updated date from the <span> tag\n",
    "        date_published = response.xpath('//div[contains(@class, \"storyPage_date__JS9qJ\")]//span/text()').get()\n",
    "\n",
    "        # Yield or print the extracted data\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'author_name': author_name,\n",
    "            'author_url': author_url,\n",
    "            'date_published': date_published,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2201c41b-53df-4501-bc04-9abdbfc7d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 01:19:55 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-09-13 01:19:55 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-6.8.0-40-generic-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess(settings={\n",
    "    \"FEEDS\": {\n",
    "        \"articles.json\": {\"format\": \"json\"},\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042aa05c-7657-429b-8074-1fdd37da7256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 01:19:55 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-09-13 01:19:55 [py.warnings] WARNING: /home/aditya/.local/lib/python3.10/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-09-13 01:19:56 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2024-09-13 01:19:56 [scrapy.extensions.telnet] INFO: Telnet Password: ce5fea6fec98c16e\n",
      "2024-09-13 01:19:56 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-09-13 01:19:56 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-09-13 01:19:56 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-09-13 01:19:56 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-09-13 01:19:56 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-09-13 01:19:56 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-09-13 01:19:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-09-13 01:19:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2024-09-13 01:19:56 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://www.livemint.com/companies/birla-estates-buys-land-from-hindalco-industries-for-rs-537-42-crore-in-mumbai-11726159806501.html> (referer: None)\n",
      "2024-09-13 01:19:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.livemint.com/companies/birla-estates-buys-land-from-hindalco-industries-for-rs-537-42-crore-in-mumbai-11726159806501.html>: HTTP status code is not handled or not allowed\n",
      "2024-09-13 01:19:56 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-09-13 01:19:56 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: articles.json\n",
      "2024-09-13 01:19:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 325,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 1104,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 0.381559,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 9, 12, 19, 49, 56, 656146, tzinfo=datetime.timezone.utc),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 12,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 144044032,\n",
      " 'memusage/startup': 144044032,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 9, 12, 19, 49, 56, 274587, tzinfo=datetime.timezone.utc)}\n",
      "2024-09-13 01:19:56 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "process.crawl(ExampleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac170b-6c87-4509-9859-b4dbd4e47514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
